# â˜ï¸ Projeto Cloud Native & Serverless - Assistente de ConversaÃ§Ã£o Inteligente

Este repositÃ³rio contÃ©m a implementaÃ§Ã£o completa do **Trabalho PrÃ¡tico 2**, apresentando um serviÃ§o de chat resiliente e escalÃ¡vel que utiliza uma arquitetura hÃ­brida (Serverless + Containers).

## ğŸ“‹ Integrantes
* **InstituiÃ§Ã£o:** PUC Minas
* **Curso:** Arquitetura de SoluÃ§Ãµes
* **Grupo:**
  * Aline Maria - [MatrÃ­cula]
  * Cristiana Elisa - [MatrÃ­cula]
  * Davi Felipe - [MatrÃ­cula]
  * Guilherme Gabriel - [MatrÃ­cula]

---

## ğŸ—ï¸ Arquitetura e Fluxo de Dados

A soluÃ§Ã£o combina a agilidade do Serverless para o Front-end/API com a robustez de Containers (Workers) para o processamento pesado de IA.

1.  **Entrada:** UsuÃ¡rio envia mensagem via Frontend â†’ **API Gateway**.
2.  **IngestÃ£o (FaaS):** Lambda recebe o request, valida e publica na fila **SQS** para processamento assÃ­ncrono.
3.  **Processamento (Worker):**
    * O componente **Worker** consome a fila SQS.
    * Recupera o contexto da conversa no **Redis** (Cache) ou **DynamoDB**.
    * Realiza a chamada Ã  API de LLM (OpenAI/Anthropic).
4.  **Resposta:** O Worker envia a resposta gerada diretamente ao cliente via conexÃ£o **WebSocket**.

<img width="903" height="592" alt="image" src="https://github.com/user-attachments/assets/f4645117-2b04-4123-b72e-4a5a267d2d29" />

(Imagem: Trabalho de Arquitetura de SoluÃ§Ãµes Cloud Native & Serverless.doc)

---

## âš™ï¸ Detalhes da ImplementaÃ§Ã£o: O Worker

O **Worker** Ã© o coraÃ§Ã£o do processamento desta aplicaÃ§Ã£o. Diferente das funÃ§Ãµes Serverless (que possuem tempo de vida curto), o Worker roda em container para gerenciar conexÃµes longas e processamento complexo sem risco de *timeout*.

* **LocalizaÃ§Ã£o:** `/src/worker`
* **Tecnologia:** Python / Node.js
* **Responsabilidades:**
    * Consumo escalÃ¡vel da fila SQS.
    * OrquestraÃ§Ã£o da chamada Ã  IA.
    * Gerenciamento de estado (State Management) das mensagens.

---

## ğŸ›¡ï¸ ResiliÃªncia e Melhorias (Novidades)

Nesta versÃ£o, implementamos padrÃµes robustos de resiliÃªncia para garantir que o serviÃ§o continue funcionando mesmo com instabilidades na API de IA.

### 1. Circuit Breaker
Implementado no `Worker` para proteger o sistema contra falhas na API externa (LLM).
* **Funcionamento:** Se a API da OpenAI/Anthropic comeÃ§ar a falhar repetidamente (ex: > 5 erros em 10s), o circuito "abre" e o Worker para de tentar enviar requisiÃ§Ãµes temporariamente, retornando um erro amigÃ¡vel imediatamente ("Fail Fast"). Isso evita o consumo desnecessÃ¡rio de recursos e custos.
* *Status:* âœ… Implementado e testado.

### 2. Retries com Exponential Backoff
Na leitura da fila SQS.
* **Funcionamento:** Caso ocorra um erro transiente (ex: falha de rede momentÃ¢nea), a mensagem nÃ£o Ã© perdida. Ela retorna Ã  fila e Ã© processada novamente apÃ³s um intervalo de tempo crescente (2s, 4s, 8s...), garantindo eventual consistÃªncia.

### 3. Dead Letter Queue (DLQ)
* **Funcionamento:** Mensagens que falham apÃ³s `N` tentativas sÃ£o movidas para uma fila segregada (DLQ) para anÃ¡lise manual, garantindo que nenhum dado do cliente seja perdido silenciosamente.

---

## ğŸ“Š Observabilidade

A aplicaÃ§Ã£o agora conta com instrumentaÃ§Ã£o para monitoramento em tempo real.

* **Traces:** Rastreamento distribuÃ­do (FaaS â†’ SQS â†’ Worker) para identificar gargalos de latÃªncia.
* **MÃ©tricas:** Monitoramento de:
    * *Throughput* de mensagens na fila.
    * Taxa de erros no Circuit Breaker.
    * LatÃªncia da API de LLM.
* **Logs Estruturados:** Logs em formato JSON para fÃ¡cil ingestÃ£o e busca.

---

## ğŸš€ Como executar o Worker localmente

1.  Configure as variÃ¡veis de ambiente:
    ```bash
    cp .env.example .env
    # Preencha suas chaves da AWS e OpenAI API Key
    ```
2.  Instale as dependÃªncias:
    ```bash
    cd src/worker
    npm install  # ou pip install -r requirements.txt
    ```
3.  Inicie o serviÃ§o:
    ```bash
    npm start    # ou python worker.py
    ```

---

## ğŸ’° CloudOps & FinOps

* **Infraestrutura como CÃ³digo (IaC):** Todo o ambiente (Filas, Tabelas, Lambdas) Ã© provisionado automaticamente.
* **Controle de Custos:** O uso de filas SQS permite "achatar" a curva de requisiÃ§Ãµes, evitando que picos de trÃ¡fego disparem custos excessivos de concorrÃªncia na LLM.

---

## ğŸ› ï¸ Como rodar o projeto localmente
  
 **PrÃ©-requisitos**
 
  Docker & Docker Compose
  
  Node.js v18+ / Python 3.9+
  
  Conta configurada na AWS (CLI)

  **Passos**
  
1.  Clone o repositÃ³rio:
    ```bash
    git clone https://github.com/guizombas/cloud-ia.git
    ```
2.  Instale as dependÃªncias:
    ```bash
    npm install
    ```
3.  Deploy da infraestrutura:
    ```bash
    serverless deploy --stage dev
    ```

---

## Tolist

Prioridades:
- [x] Criar base para criaÃ§Ã£o de lambdas
- [x] Criar lambda de POST de mensagem (gerar jobId, enviar para fila SQS e salvar connection id no redis)
- [ ] Criar worker pod que lÃª do SQS
- [ ] No worker pod, implementar chamada para a API LLM (passar API_KEY no .env)
- [ ] No worker pod, implementar leitura e escrita de mensagens da conversa no dynamodb
- [ ] No worker pod, implementar retorno no websocket lendo connection id do redis
- [ ] Criar serviÃ§o que sobe conexÃ£o websocket (nÃ£o tem API Gateway WebSocket no Localstack free)
- [ ] Gerar connectionId e retornar na conexÃ£o no serviÃ§o websocket

Menor prioridade:
- [ ] Usar o parameter store para salvar API_KEY
- [ ] implementar refresh de sessÃ£o websocket 
- [ ] implementar kubernets local usando helm
- [ ] Criar lambda de GET de mensagens
- [ ] Criar lamda de GET de conversas
- [ ] Implementar cache de mensagens no redis no worker e no 
- [ ] Detalhar readme com arquitetura e diagrama e contexto
- [ ] Criar frontend que faz as chamadas
- [ ] Adicionar instrumentaÃ§Ã£o no newRelic
- [ ] Fazer parte de infra como cÃ³digo (terraform)
