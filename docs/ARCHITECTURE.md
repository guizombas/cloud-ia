# ğŸ—ï¸ Arquitetura de SoluÃ§Ãµes: Cloud-IA

Este documento detalha o design arquitetural do projeto **cloud-ia**. A soluÃ§Ã£o adota um modelo **Cloud Native, HÃ­brido e Orientado a Eventos** para resolver o desafio de processar requisiÃ§Ãµes de IA Generativa com alta latÃªncia sem bloquear o cliente.

---

## 1. Diagrama de Fluxo de Dados

<img width="931" height="581" alt="image" src="https://github.com/user-attachments/assets/ac845f4d-1a4a-4090-85c4-e1b3fb2faae3" />

O fluxo da aplicaÃ§Ã£o Ã© dividido em dois canais: **Entrada (HTTP)** e **SaÃ­da (WebSocket)**.

1.  **Cliente (Frontend)** conecta no **WebSocket** (`$connect`) â†’ `ConnectionId` Ã© salvo no **Redis**.
2.  **Cliente** envia mensagem via **API Gateway HTTP** (`POST /chat`).
3.  **Lambda (FaaS)** recebe a requisiÃ§Ã£o, valida, gera um `JobId` e publica na fila **SQS**. Responde HTTP 202 (Accepted) imediatamente.
4.  **Worker (Container)** consome a mensagem da **SQS**.
5.  **Worker** recupera contexto no **DynamoDB/Redis** e chama a **LLM API** (OpenAI/Anthropic).
6.  **Worker** publica a resposta da IA para o **WebSocket Service** usando o `ConnectionId` recuperado do Redis.
7.  **Cliente** recebe a resposta em tempo real.

---

## 2. Componentes e DecisÃµes TÃ©cnicas

### 2.1. Camada HÃ­brida de ComputaÃ§Ã£o
A decisÃ£o de separar FaaS e Containers foi estratÃ©gica:

* **FaaS (Lambdas):** Usado para tarefas rÃ¡pidas e *stateless* (receber mensagem HTTP, gerenciar conexÃ£o WS). Escala a zero para economizar custos.
* **Containers (Worker Pods):** Usado para o processamento "pesado". MantÃ©m conexÃµes longas, processa filas continuamente e evita os limites de *timeout* (29s) das Lambdas ao aguardar a resposta da LLM.

### 2.2. Mensageria e Assincronismo (SQS)
A fila **Amazon SQS** atua como buffer de *backpressure*. Se houver um pico de 10.000 requisiÃ§Ãµes, as Lambdas enfileiram tudo rapidamente, e os Workers processam na velocidade que a API da IA suportar, sem derrubar o sistema.
* *ReferÃªncia:* Veja detalhes de retries na [DocumentaÃ§Ã£o de ResiliÃªncia](./RESILIENCE.md).

### 2.3. EstratÃ©gia de Dados (Hot vs Cold)
* **Redis (Hot Data):** Armazena o mapeamento `SessionId` â†” `ConnectionId` e cache de contexto de curto prazo. NecessÃ¡rio para roteamento de mensagens com latÃªncia de milissegundos.
* **DynamoDB (Cold/Persist Data):** Banco NoSQL Single-Table para histÃ³rico de chat. Escolhido pela capacidade de escalar *throughput* instantaneamente.

---

## 3. ResiliÃªncia e Falhas

A arquitetura implementa o padrÃ£o **Fail Fast** e proteÃ§Ãµes contra falhas externas.

* **Circuit Breaker:** Implementado no Worker. Se a API da LLM falhar repetidamente, o circuito abre e o Worker para de tentar, economizando recursos.
* **DLQ (Dead Letter Queue):** Mensagens que falham apÃ³s N tentativas (ex: erro de *parsing*) sÃ£o movidas para anÃ¡lise manual, garantindo **Zero Data Loss**.

> Para uma lista completa dos mecanismos (Timeouts, Backoff, etc), consulte [RESILIENCE.md](./RESILIENCE.md).

---

## 4. Interfaces e Contratos

A comunicaÃ§Ã£o entre os serviÃ§os segue contratos estritos definidos na documentaÃ§Ã£o da API.

* **Entrada:** REST via `POST /chat`
* **SaÃ­da:** Eventos WebSocket assÃ­ncronos (`message_completed`, `error`).

> Os payloads JSON e cÃ³digos de erro estÃ£o documentados em [API.md](./API.md).

---

## 5. Deployment e Infraestrutura

O projeto suporta deployment local (via Docker Compose + Serverless Offline) e em produÃ§Ã£o (AWS + Kubernetes).

* **Local:** Um serviÃ§o customizado em Node.js simula o WebSocket da AWS, pois o LocalStack Free nÃ£o oferece suporte a API Gateway V2.
* **ProduÃ§Ã£o:** API Gateway nativo gerencia as conexÃµes WebSocket.

> InstruÃ§Ãµes de deploy detalhadas em [DEPLOYMENT.md](./DEPLOYMENT.md).
